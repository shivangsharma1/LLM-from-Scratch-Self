{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x107d4bf50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2961, 0.5166, 0.2517, 0.6886, 0.0740],\n",
       "        [0.8665, 0.1366, 0.1025, 0.1841, 0.7264],\n",
       "        [0.3153, 0.6871, 0.0756, 0.1966, 0.3164],\n",
       "        [0.4017, 0.1186, 0.8274, 0.3821, 0.6605],\n",
       "        [0.8536, 0.5932, 0.6367, 0.9826, 0.2745]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_tensor = torch.rand(5, 5)\n",
    "in_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attn_score\n",
    "attn_score = in_tensor @ in_tensor\n",
    "# norm_attn = attn_score/attn_score.sum(dim=0)\n",
    "# norm_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2361, 0.1532, 0.1950, 0.1802, 0.2355],\n",
       "        [0.2166, 0.1937, 0.1693, 0.2997, 0.1207],\n",
       "        [0.2752, 0.1601, 0.1601, 0.2004, 0.2042],\n",
       "        [0.2214, 0.2280, 0.1661, 0.2343, 0.1501],\n",
       "        [0.2399, 0.1678, 0.1804, 0.2108, 0.2011]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weight = torch.softmax(attn_score, dim = -1)\n",
    "attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5375, 0.4379, 0.3889, 0.5294, 0.3741],\n",
       "        [0.5088, 0.3618, 0.4120, 0.4512, 0.4414],\n",
       "        [0.5255, 0.4189, 0.3936, 0.5277, 0.3757],\n",
       "        [0.5378, 0.3765, 0.3811, 0.4641, 0.4305],\n",
       "        [0.5296, 0.4151, 0.3937, 0.5097, 0.3911]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs = attn_weight @ in_tensor\n",
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention with  Trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = in_tensor[1]\n",
    "d_in = in_tensor.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "torch.Size([5, 2])\n",
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "#the w, q, v matrices are initialized as parameter of the attention module\n",
    "\n",
    "Q = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "K = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "V = nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "#setting require_grad = False because not training, if training set as True\n",
    "\n",
    "print(Q.shape)\n",
    "print(K.shape)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query : tensor([[1.2085, 1.3420],\n",
      "        [1.3403, 1.2556],\n",
      "        [1.1682, 1.2064],\n",
      "        [1.1469, 1.8641],\n",
      "        [2.0150, 2.3172]])\n",
      "Key : tensor([[0.6785, 0.5486],\n",
      "        [1.2294, 0.5470],\n",
      "        [0.7758, 0.3918],\n",
      "        [1.3894, 0.8629],\n",
      "        [1.4164, 0.9855]])\n",
      "Value : tensor([[1.1248, 1.2415],\n",
      "        [1.2242, 0.9943],\n",
      "        [0.9601, 1.0932],\n",
      "        [1.6329, 1.3659],\n",
      "        [2.0849, 2.0791]])\n"
     ]
    }
   ],
   "source": [
    "query = in_tensor @ Q\n",
    "key = in_tensor @ K\n",
    "value = in_tensor @ V\n",
    "\n",
    "print(f\"Query : {query}\")\n",
    "print(f\"Key : {key}\")\n",
    "print(f\"Value : {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5562, 2.2198, 1.4633, 2.8371, 3.0344],\n",
       "        [1.5982, 2.3345, 1.5317, 2.9456, 3.1358],\n",
       "        [1.4544, 2.0960, 1.3789, 2.6640, 2.8436],\n",
       "        [1.8008, 2.4295, 1.6200, 3.2019, 3.4615],\n",
       "        [2.6384, 3.7446, 2.4710, 4.7990, 5.1377]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_score = query @ key.T\n",
    "attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1130, 0.1806, 0.1058, 0.2794, 0.3213],\n",
       "        [0.1087, 0.1830, 0.1037, 0.2819, 0.3225],\n",
       "        [0.1170, 0.1842, 0.1109, 0.2753, 0.3125],\n",
       "        [0.1067, 0.1665, 0.0939, 0.2875, 0.3454],\n",
       "        [0.0688, 0.1504, 0.0611, 0.3170, 0.4027]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating dimension of key to normalize the vector before calculating softmax\n",
    "\n",
    "# reason for normalizing (dividing by key dimension): Improve the training performance, by avoiding small gradients. Let's say when scaling up the embedding dim\n",
    "# which is typically greater in LLM ~1000 like in GPT, the dot product can result in very small gradients during backprop because of the softmax applied to them.\n",
    "\n",
    "dim = key.shape[-1]\n",
    "attn_weight2 = torch.softmax(attn_score/(dim ** 0.5), dim=-1)\n",
    "attn_weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5758, 1.4850],\n",
       "        [1.5789, 1.4861],\n",
       "        [1.5648, 1.4755],\n",
       "        [1.6036, 1.5115],\n",
       "        [1.6774, 1.5720]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec = attn_weight2 @ value\n",
    "context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
